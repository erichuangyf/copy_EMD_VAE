{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on B-jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "#from scipy import linalg as LA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.tf_sinkhorn import ground_distance_tf_nograd, sinkhorn_knopp_tf_scaling_stabilized_class\n",
    "import utils.VAE_model_tools_leakyrelu\n",
    "from utils.VAE_model_tools_leakyrelu import build_and_compile_annealing_vae, betaVAEModel, reset_metrics\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    ''' Creates a directory (or nested directories) if they don't exist.\n",
    "    '''\n",
    "    if not osp.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    return dir_path\n",
    "\n",
    "def ptetaphiE_to_Epxpypz(jets):\n",
    "    pt = jets[:,:,0]\n",
    "    eta = jets[:,:,1]\n",
    "    phi = jets[:,:,2]\n",
    "    E = jets[:,:,3]\n",
    "    \n",
    "    px = pt * np.cos(phi)\n",
    "    py = pt * np.sin(phi)\n",
    "    pz = pt * np.sinh(eta)\n",
    "    \n",
    "    newjets = np.zeros(jets.shape)\n",
    "    newjets[:,:,0] = E\n",
    "    newjets[:,:,1] = px\n",
    "    newjets[:,:,2] = py\n",
    "    newjets[:,:,3] = pz\n",
    "    \n",
    "    return newjets\n",
    "\n",
    "def ptetaphiE_to_ptyphim(jets):\n",
    "    pt = jets[:,:,0]\n",
    "    eta = jets[:,:,1]\n",
    "    phi = jets[:,:,2]\n",
    "    E = jets[:,:,3]\n",
    "    \n",
    "    pz = pt * np.sinh(eta)\n",
    "    y = 0.5*np.nan_to_num(np.log((E+pz)/(E-pz)))\n",
    "    \n",
    "    msqr = np.square(E)-np.square(pt)-np.square(pz)\n",
    "    msqr[np.abs(msqr) < 1e-6] = 0\n",
    "    m = np.sqrt(msqr)\n",
    "    \n",
    "    newjets = np.zeros(jets.shape)\n",
    "    newjets[:,:,0] = pt\n",
    "    newjets[:,:,1] = y\n",
    "    newjets[:,:,2] = phi\n",
    "    newjets[:,:,3] = m\n",
    "    \n",
    "    return newjets\n",
    "    \n",
    "def ptyphim_to_ptetaphiE(jets):\n",
    "    \n",
    "    pt = jets[:,:,0]\n",
    "    y = jets[:,:,1]\n",
    "    phi = jets[:,:,2]\n",
    "    m = jets[:,:,3]\n",
    "    \n",
    "    eta = np.nan_to_num(np.arcsinh(np.sinh(y)*np.sqrt(1+np.square(m/pt))))\n",
    "    pz = pt * np.sinh(eta)\n",
    "    E = np.sqrt(np.square(pz)+np.square(pt)+np.square(m))\n",
    "    \n",
    "    newjets = np.zeros(jets.shape)\n",
    "    newjets[:,:,0] = pt\n",
    "    newjets[:,:,1] = eta\n",
    "    newjets[:,:,2] = phi\n",
    "    newjets[:,:,3] = E\n",
    "    \n",
    "    return newjets\n",
    "    \n",
    "def center_jets_ptetaphiE(jets):\n",
    "    cartesian_jets = ptetaphiE_to_Epxpypz(jets)\n",
    "    sumjet_cartesian = np.sum(cartesian_jets,axis=1)\n",
    "    \n",
    "    sumjet_phi = np.arctan2(sumjet_cartesian[:,2],sumjet_cartesian[:,1])\n",
    "    sumjet_y = 0.5*np.log((sumjet_cartesian[:,0] + sumjet_cartesian[:,-1])/(sumjet_cartesian[:,0] - sumjet_cartesian[:,-1]))\n",
    "    \n",
    "    ptyphim_jets = ptetaphiE_to_ptyphim(jets)\n",
    "    #print(ptyphim_jets[:3,:,:])\n",
    "    \n",
    "    transformed_jets = np.copy(ptyphim_jets)\n",
    "    transformed_jets[:,:,1] = ptyphim_jets[:,:,1] - sumjet_y[:,None]\n",
    "    transformed_jets[:,:,2] = ptyphim_jets[:,:,2] - sumjet_phi[:,None]\n",
    "    transformed_jets[:,:,2] = transformed_jets[:,:,2] + np.pi\n",
    "    transformed_jets[:,:,2] = np.mod(transformed_jets[:,:,2],2*np.pi)\n",
    "    transformed_jets[:,:,2] = transformed_jets[:,:,2] - np.pi\n",
    "\n",
    "    transformed_jets[transformed_jets[:,:,0] == 0] = 0\n",
    "    \n",
    "    newjets = ptyphim_to_ptetaphiE(transformed_jets)\n",
    "    return newjets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to file\n",
    "fn =  '/global/home/users/yifengh3/VAE/data/B_background.h5'\n",
    "\n",
    "# Option 1: Load everything into memory\n",
    "df = pandas.read_hdf(fn,stop=1000000)\n",
    "print(df.shape)\n",
    "print(\"Memory in GB:\",sum(df.memory_usage(deep=True)) / (1024**3)+sum(df.memory_usage(deep=True)) / (1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file contains, for each event, 50 particles (with zero padding), each particle with pT, eta, phi\n",
    "data = df.values.reshape((-1,50,3))\n",
    "\n",
    "# Normalize pTs so that HT = 1\n",
    "HT = np.sum(data[:,:,0],axis=-1)\n",
    "data[:,:,0] = data[:,:,0]/HT[:,None]\n",
    "# Energy column DNE here, so dont need to normalize the data\n",
    "# data[:,:,-1] = data[:,:,-1]/HT[:,None]\n",
    "\n",
    "# Center jet (optional)\n",
    "# cant center the jets since E missing here\n",
    "# data = center_jets_ptetaphiE(data)\n",
    "\n",
    "# Inputs x to NN will be: pT, eta, cos(phi), sin(phi), log E\n",
    "# Separated phi into cos and sin for continuity around full detector, so make things easier for NN.\n",
    "# Also adding the log E is mainly because it seems like it should make things easier for NN, since there is an exponential spread in particle energies.\n",
    "# Feel free to change these choices as desired. E.g. px, py might be equally as good as pt, sin, cos.\n",
    "sig_input = np.zeros((len(data),50,4))\n",
    "sig_input[:,:,:2] = data[:,:,:2]\n",
    "sig_input[:,:,2] = np.cos(data[:,:,2])\n",
    "sig_input[:,:,3] = np.sin(data[:,:,2])\n",
    "# no input from energy for B jets\n",
    "# sig_input[:,:,4] = np.log(data[:,:,3]+1e-8)\n",
    "\n",
    "\n",
    "data_x = sig_input\n",
    "# Event 'labels' y are [pT, eta, phi], which is used to calculate EMD to output which is also pT, eta, phi.\n",
    "data_y = data\n",
    "\n",
    "\n",
    "train_x = data_x[:800000]\n",
    "train_y = data_y[:800000]\n",
    "valid_x = data_x[800000:]\n",
    "valid_y = data_y[800000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_dir = '/global/home/users/yifengh3/VAE/B_results'\n",
    "experiment_name = 'method2_beta1'\n",
    "train_output_dir = create_dir(osp.join(output_dir, experiment_name))\n",
    "vae_args_file = osp.join(train_output_dir,\"vae_args.dat\")\n",
    "vae_arg_dict = None\n",
    "\n",
    "# if the arg file exists, load model config\n",
    "if os.path.isfile(vae_args_file):\n",
    "    with open(vae_args_file,'r') as f:\n",
    "          vae_arg_dict = json.loads(f.read())\n",
    "else:\n",
    "    # otherwize use the initial argument dict\n",
    "    vae_arg_dict = {\"encoder_conv_layers\" : [2048,2048,1024,1024],\n",
    "                    \"dense_size\" :[1028,1028,1028,512],\n",
    "                    \"decoder\" : [4096,2048,1028,512,512],\n",
    "                    \"numItermaxinner\" : 40,   # EMD approximation params\n",
    "                    \"numIter\":10,\n",
    "                    \"reg_init\" : 1.,\n",
    "                    \"reg_final\" : 0.01,\n",
    "                    \"stopThr\":1e-3,\n",
    "                    \"num_inputs\":4,           # Size of x (e.g. pT, eta, sin, cos, log E)\n",
    "                    \"num_particles_in\":50}\n",
    "    with open(vae_args_file,'w') as file:\n",
    "      file.write(json.dumps(vae_arg_dict))\n",
    "    \n",
    "vae, encoder, decoder = build_and_compile_annealing_vae(**vae_arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_set = np.logspace(-5,1,25)[:-5]\n",
    "# betas = beta_set\n",
    "# for i in range(0,16,2):\n",
    "#   betas = np.append(betas, beta_set[-1-7-i:-1-i-1])\n",
    "#   betas = np.append(betas, beta_set[-1-7-i:-1-i][::-1])\n",
    "# betas = np.append(betas, beta_set[1:])\n",
    "# plt.plot(betas)\n",
    "# plt.title(\"old beta scheduling\")\n",
    "# plt.semilogy()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_set = np.logspace(-5,1,25)[:-3]\n",
    "betas = beta_set\n",
    "for i in range(0,16,2):\n",
    "  betas = np.append(betas, beta_set[-1-7-i:-1-i])\n",
    "betas = np.append(betas, beta_set)\n",
    "plt.plot(betas)\n",
    "plt.title(\"new beta scheduling\")\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init_epoch = 0\n",
    "steps_per_epoch = 1000\n",
    "batch_size=100\n",
    "save_period=40\n",
    "\n",
    "# define some directory so the model file will not appears everywhere\n",
    "checkpoint_dir = create_dir(osp.join(train_output_dir, \"checkpoint\"))\n",
    "end_beta_checkpoint = create_dir(osp.join(train_output_dir, \"end_beta_checkpoint\"))\n",
    "\n",
    "modelcheckpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_dir + '/model_weights_{epoch:02d}.hdf5', save_freq = save_period*5000, save_weights_only=True)\n",
    "\n",
    "reset_metrics_inst = reset_metrics()\n",
    "\n",
    "reduceLR = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=np.sqrt(0.1), \n",
    "    patience=5, verbose=1, mode='auto', \n",
    "    min_delta=1e-4, cooldown=0, min_lr=1e-8)\n",
    "\n",
    "\n",
    "callbacks=[tf.keras.callbacks.CSVLogger(train_output_dir + '/log.csv', separator=\",\", append=True),\n",
    "            reduceLR,\n",
    "            modelcheckpoint,\n",
    "            reset_metrics_inst]\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0., patience=5, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")\n",
    "\n",
    "for beta in betas:\n",
    "    vae.beta.assign(beta)\n",
    "    K.set_value(vae.optimizer.lr,3e-5)\n",
    "    \n",
    "    my_history = vae.fit(x=train_x, y=train_y, batch_size=batch_size,\n",
    "                epochs=10000,verbose=1,\n",
    "                validation_data = (valid_x[:200*batch_size],valid_y[:200*batch_size]),\n",
    "                callbacks = callbacks,\n",
    "                initial_epoch=init_epoch,\n",
    "                steps_per_epoch = steps_per_epoch\n",
    "              )\n",
    "    \n",
    "    init_epoch = my_history.epoch[-1]\n",
    "    \n",
    "    vae.save_weights(osp.join(\n",
    "        end_beta_checkpoint, \n",
    "        '/model_weights_end_' + str(init_epoch) + '_' + \"{:.1e}\".format(beta) + '.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
