{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115dd059",
   "metadata": {},
   "source": [
    "# VAE sampler (generate sample from VAE Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb52e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/yifengh3/VAE/EMD_VAE\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a183fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 0 Logical GPUs\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from utils.tf_sinkhorn import ground_distance_tf_nograd, sinkhorn_knopp_tf_scaling_stabilized_class\n",
    "import utils.VAE_model_tools_leakyrelu\n",
    "from utils.VAE_model_tools_leakyrelu import build_and_compile_annealing_vae, betaVAEModel, reset_metrics\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a429412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 150)\n",
      "Memory in GB: 2.250075340270996\n"
     ]
    }
   ],
   "source": [
    "# Now load some data\n",
    "# path to file\n",
    "fn =  '/global/home/users/yifengh3/VAE/data/B_background.h5'\n",
    "\n",
    "df = pandas.read_hdf(fn,stop=1000000)\n",
    "print(df.shape)\n",
    "print(\"Memory in GB:\",sum(df.memory_usage(deep=True)) / (1024**3)+sum(df.memory_usage(deep=True)) / (1024**3))\n",
    "\n",
    "# Data file contains, for each event, 50 particles (with zero padding), each particle with pT, eta, phi\n",
    "data = df.values.reshape((-1,50,3))\n",
    "\n",
    "# Normalize pTs so that HT = 1\n",
    "HT = np.sum(data[:,:,0],axis=-1)\n",
    "data[:,:,0] = data[:,:,0]/HT[:,None]\n",
    "\n",
    "# Inputs x to NN will be: pT, eta, cos(phi), sin(phi), log E\n",
    "# Separated phi into cos and sin for continuity around full detector, so make things easier for NN.\n",
    "# Also adding the log E is mainly because it seems like it should make things easier for NN, since there is an exponential spread in particle energies.\n",
    "# Feel free to change these choices as desired. E.g. px, py might be equally as good as pt, sin, cos.\n",
    "sig_input = np.zeros((len(data),50,4))\n",
    "sig_input[:,:,:2] = data[:,:,:2]\n",
    "sig_input[:,:,2] = np.cos(data[:,:,2])\n",
    "sig_input[:,:,3] = np.sin(data[:,:,2])\n",
    "# no input from energy for B jets\n",
    "# sig_input[:,:,4] = np.log(data[:,:,3]+1e-8)\n",
    "\n",
    "\n",
    "data_x = sig_input\n",
    "# Event 'labels' y are [pT, eta, phi], which is used to calculate EMD to output which is also pT, eta, phi.\n",
    "data_y = data\n",
    "\n",
    "\n",
    "train_x = data_x[:800000]\n",
    "train_y = data_y[:800000]\n",
    "valid_x = data_x[800000:]\n",
    "valid_y = data_y[800000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b8c667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " vae_arg_dict: {'encoder_conv_layers': [1024, 1024, 1024, 1024], 'dense_size': [1024, 1024, 1024, 1024], 'decoder_sizes': [1024, 1024, 1024, 1024, 1024], 'numItermaxinner': 20, 'numIter': 10, 'reg_init': 1.0, 'reg_final': 0.01, 'stopThr': 0.001, 'num_inputs': 4, 'num_particles_in': 50}\n",
      "WARNING:tensorflow:From /global/home/users/yifengh3/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 50, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 50, 1024)     5120        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 50, 1024)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 50, 1024)     1049600     leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 50, 1024)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 50, 1024)     1049600     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 50, 1024)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 50, 1024)     1049600     leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 50, 1024)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 1024)         0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         1049600     tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1024)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         1049600     leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         1049600     leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 128)          131200      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 128)          131200      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            multiple             7614720     inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack (TFOpLambda)           (2, None, 128)       0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 50, 3)        4535496     encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gauss_distribution (Dis multiple             0           tf.stack[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,150,217\n",
      "Trainable params: 12,150,216\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load and build the model\n",
    "model_prefix = \"/global/home/users/yifengh3/VAE/B_results/method2_beta1\"\n",
    "\n",
    "import json\n",
    "vae_args_file = osp.join(model_prefix,\"vae_args.dat\")\n",
    "with open(vae_args_file,'r') as f:\n",
    "  vae_arg_dict = json.loads(f.read())\n",
    "\n",
    "print(\"\\n\\n vae_arg_dict:\", vae_arg_dict)\n",
    "\n",
    "vae, encoder, decoder = build_and_compile_annealing_vae(**vae_arg_dict)\n",
    "\n",
    "batch_size=150\n",
    "save_period=2\n",
    "\n",
    "vae.beta.assign(0.001)\n",
    "\n",
    "K.set_value(vae.optimizer.lr,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39e9124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outjets(weight_file, validation_data_input, number_of_sampling=3,):\n",
    "    \"\"\"Generate outjets from VAE model\n",
    "        INPUT:\n",
    "        weight file: weight file location for VAE model\n",
    "        validation_data_input: validation data input, stored as numpy array\n",
    "        number_of_sampling: number of sampling, default to 3\n",
    "        OUTPUT:\n",
    "        outs_jet: numpy array of outjets, shape: (number_of_sampling x data_inputsize)\n",
    "    \"\"\"\n",
    "    vae.load_weights(weight_file)\n",
    "    outs_jet = np.stack([vae.predict(validation_data_input)[0] for j in range(number_of_sampling)])\n",
    "    return outs_jet\n",
    "\n",
    "def save_as_hdf5(original_jets,sampled_jets, file_prefix=\".\", savename = \"jets.h5\"):\n",
    "    \"\"\"Save data as hdf5 file\n",
    "        original_jets: original input jets\n",
    "        sampled_jets: sampled jets from VAE model\n",
    "        file_prefix: output file prefix\n",
    "        savename: output file name\n",
    "    \"\"\"\n",
    "    hfile = h5py.File(osp.join(file_prefix,savename), 'w')\n",
    "    hfile.create_dataset('original_jets', data=original_jets)\n",
    "    hfile.create_dataset('sampled_jets', data=sampled_jets)\n",
    "    hfile.close()\n",
    "\n",
    "def generate_samples(weight_file, valid_input, originial_jet, number_of_sampling=3,\n",
    "                     start_index=0, stop_index=1000, outfile_prefix=\".\", outfile_name=\"jets.h5\"):\n",
    "    \n",
    "    jets = generate_outjets(weight_file, valid_x[start_index:stop_index], number_of_sampling=number_of_sampling)\n",
    "    save_as_hdf5(originial_jet[start_index:stop_index],jets, \n",
    "                 file_prefix=outfile_prefix, savename = outfile_name)\n",
    "    return jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a7e4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = osp.join(model_prefix,\"checkpoint/model_weights_400.hdf5\")\n",
    "samples = generate_samples(weight_file, valid_x, valid_y, stop_index=2000, \n",
    "                 outfile_prefix=\"/global/home/users/yifengh3/VAE/B_results/method2_beta1\",outfile_name=\"sampled_jets.h5\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
