{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3317a65a",
   "metadata": {},
   "source": [
    "# B jet study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc38d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/yifengh3/EMD_VAE\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac299899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 0 Logical GPUs\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.tf_sinkhorn import ground_distance_tf_nograd, sinkhorn_knopp_tf_scaling_stabilized_class\n",
    "import utils.VAE_model_tools\n",
    "from utils.VAE_model_tools import build_and_compile_annealing_vae, betaVAEModel, reset_metrics\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb8b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    ''' Creates a directory (or nested directories) if they don't exist.\n",
    "    '''\n",
    "    if not osp.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    return dir_path\n",
    "\n",
    "output_dir = './data/'\n",
    "    \n",
    "def kl_loss(z_mean, z_log_var):\n",
    "    return -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "    \n",
    "    \n",
    "from pyjet import cluster\n",
    "\n",
    "def get_clustered_pt_eta_phi(pts, locations,R=0.1):\n",
    "    weights = pts\n",
    "    outjet = locations\n",
    "    myjet = np.zeros((weights.shape[-1]),dtype=([('pT', 'f8'), ('eta', 'f8'), ('phi', 'f8'), ('mass', 'f8')]))\n",
    "    myjet['pT'] = weights\n",
    "    myjet['eta'] = outjet[:,0]\n",
    "    myjet['phi'] = outjet[:,1]\n",
    "    sequence = cluster(myjet,R=R,p=0)\n",
    "    jets = sequence.inclusive_jets()\n",
    "    phis = np.array([np.mod(np.pi+jet.phi,2*np.pi)-np.pi for jet in jets])\n",
    "    etas = np.array([jet.eta for jet in jets])\n",
    "    pts = np.array([jet.pt for jet in jets])\n",
    "    \n",
    "    return pts, etas, phis\n",
    "\n",
    "\n",
    "def plot_jets(outs_array, numplot = 3, R=0.02,size=50, save_dir = None):\n",
    "    etalim=5\n",
    "    #bins=np.linspace(-lim, lim, 126)\n",
    "\n",
    "    for i in range(numplot):   \n",
    "\n",
    "        fig, ax = plt.subplots(1, 3,figsize=[15,5],sharey=True)\n",
    "\n",
    "\n",
    "\n",
    "        outjet = valid_y[i,:,1:]\n",
    "        weights = valid_y[i,:,0]\n",
    "        pts, etas, phis = get_clustered_pt_eta_phi(weights, outjet,R=R)\n",
    "        ax[0].scatter(phis, etas, s = pts*size, alpha = 0.7,linewidths=0)\n",
    "        ax[0].set_title('Jet'+str(i),y=0.9)\n",
    "\n",
    "        #ax[0].hist2d(feed_pc[i][:,0],feed_pc[i][:,1],range=[[-lim,lim],[-lim,lim]],bins=bins, norm=LogNorm(0.5, 1000))\n",
    "        for j in range(2):\n",
    "            outjet = outs_array[j][0][i,:,1:]\n",
    "            weights = outs_array[j][0][i,:,0]\n",
    "            pts, etas, phis = get_clustered_pt_eta_phi(weights, outjet,R=R)\n",
    "            ax[j+1].scatter(phis, etas, s = pts*size, alpha = 0.7,linewidths=0)\n",
    "            ax[j+1].set_title('Sample'+ str(j),y=0.9)\n",
    "            \n",
    "        for j in range(3):\n",
    "            ax[j].set_ylabel(r'$\\eta$',fontsize=18)\n",
    "            ax[j].set_xlabel(r'$\\phi$',fontsize=18)\n",
    "            ax[j].set_ylim([-5,5])\n",
    "            ax[j].set_xlim([-np.pi,np.pi])\n",
    "\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        if save_dir:\n",
    "            outfile = os.path.join(save_dir,\"jets_plot_{}.png\".format(i))\n",
    "            plt.savefig(outfile)\n",
    "        plt.show()\n",
    "        \n",
    "def plot_KL_logvar(outs_array,xlim=None,ylim=None):\n",
    "    \n",
    "    _, z_mean, z_log_var, z = outs_array[0]\n",
    "\n",
    "    KL=kl_loss(z_mean, z_log_var)\n",
    "    sort_kl = np.flip(np.argsort(np.mean(KL,axis=0)))\n",
    "\n",
    "    rms_mean = np.sqrt(np.mean(np.square(z_mean),axis=0))\n",
    "\n",
    "    plt.scatter(np.mean(KL,axis=0),rms_mean,s=5.)\n",
    "\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "        \n",
    "    plt.xlabel('KL divergence')\n",
    "    plt.ylabel(r'$\\sqrt{\\left\\langle \\mu^2 \\right\\rangle}$')\n",
    "    plt.show()\n",
    "    \n",
    "    return sort_kl\n",
    "\n",
    "def get_plot(file, save_dir = None):\n",
    "    vae.load_weights(file)\n",
    "    beta = get_beta(file)\n",
    "    outs_array = [vae.predict(valid_x[:1000]) for j in range(3)]\n",
    "    output_dir = None\n",
    "    \n",
    "    if save_dir:\n",
    "        output_dir = os.path.join(save_dir,\"epoch{}_beta{}\".format(get_epoch(file),get_beta(file)))\n",
    "        create_dir(output_dir)\n",
    "    \n",
    "    vae.beta.assign(beta)\n",
    "    result = vae.test_step([valid_x[:2000].astype(np.float32),valid_y[:2000].astype(np.float32)])\n",
    "    \n",
    "    print(\"epoch = {}\".format(get_epoch(file)))\n",
    "    print(\"Beta = {}\".format(beta))\n",
    "    print(\"Loss:\", \"{:.02e}\".format(result['loss'].numpy()))\n",
    "    print(\"Recon loss:\", \"{:.02e}\".format(result['recon_loss'].numpy()))\n",
    "    print(\"KL loss:\", result['KL loss'].numpy())\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"KL divergences plot. x axis is ~ log(resolution) for resolution << 1\")\n",
    "    sort_kl = plot_KL_logvar(outs_array,[-0.1,None],[-0.1,None])\n",
    "\n",
    "    print(\"Jet sample plots. Left = original jet. Middle and right: two vae samples\")\n",
    "    plot_jets(outs_array,R=0.05,size=1000,save_dir=output_dir)\n",
    "\n",
    "    print(\"Latent space distributions in most important latent space directions\")\n",
    "    _, z_mean, z_log_var, z = outs_array[0]\n",
    "\n",
    "    plt.scatter(z_mean[:,sort_kl[0]],z_mean[:,sort_kl[1]],s=1.)\n",
    "    plt.xlabel(r'$\\sqrt{\\left\\langle \\mu_0^2 \\right\\rangle}$')\n",
    "    plt.ylabel(r'$\\sqrt{\\left\\langle \\mu_1^2 \\right\\rangle}$')\n",
    "    plt.show()\n",
    "    plt.scatter(z_mean[:,sort_kl[2]],z_mean[:,sort_kl[3]],s=1.)\n",
    "    plt.xlabel(r'$\\sqrt{\\left\\langle \\mu_2^2 \\right\\rangle}$')\n",
    "    plt.ylabel(r'$\\sqrt{\\left\\langle \\mu_3^2 \\right\\rangle}$')\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(z_mean[:,sort_kl[0]],z_log_var[:,sort_kl[0]],s=1.)\n",
    "    plt.xlabel(r'$\\sqrt{\\left\\langle \\mu_0^2 \\right\\rangle}$')\n",
    "    plt.ylabel(r'$\\left\\langle \\log \\sigma_0 \\right\\rangle$')\n",
    "    plt.show()\n",
    "    plt.scatter(z_mean[:,sort_kl[1]],z_log_var[:,sort_kl[1]],s=1.)\n",
    "    plt.xlabel(r'$\\sqrt{\\left\\langle \\mu_1^2 \\right\\rangle}$')\n",
    "    plt.ylabel(r'$\\left\\langle \\log \\sigma_1 \\right\\rangle$')\n",
    "    plt.show()\n",
    "\n",
    "#     print(\"Investigation of first two latent directions. Salmon is unpopulated regions of the latent space (see scatter plots above).Note that opposite points on the disk are very similar, which reflects the topology of 2-body decays. A two body decay is specified by two coordinates, phi and theta (in the rest frame) or z (in the boosted frame) for one of the particles. This is the sphere S^2 with the phi coordinate determining longitude and z/theta determining latitude (the equator is balanced jets, the poles is maximially unbalanced). If the particles are considered identical, then antipodes are identified (which is the geometry of RP^2). The sphere is a double cover of the decay kinematics (if you know z and phi for particle 1, then you know it also for particle 2). The shape you see below is a projection of a hemisphere of that. The topology of the latent space is R^n, which cannot faithfully reproduce RP^2, but it is trying its best. Jets that are close to eachother in real space correspond to latent points that are close in the latent space, except at the rim of the disk where a small change in real space can take you straight to the opposite edge of the disk. As the resolution of the VAE gets finer and finer, this affects a smaller and smaller annulus of jets around the rim of the disk.\")\n",
    "\n",
    "    narray = 11\n",
    "    lim = 3.0\n",
    "    codes = np.zeros((narray**2,128))\n",
    "\n",
    "    dirs = [0,1]\n",
    "\n",
    "    for i in range(narray):\n",
    "        for j in range(narray):\n",
    "            codes[narray*i+j,sort_kl[dirs[0]]] = (i-(narray-1)/2)*lim/((narray-1)/2)\n",
    "            codes[narray*i+j,sort_kl[dirs[1]]] = (j-(narray-1)/2)*lim/((narray-1)/2)\n",
    "\n",
    "    decoded = decoder.predict(codes)\n",
    "\n",
    "    fig, ax = plt.subplots(narray, narray,figsize=[15,15],sharex=True,sharey=True)\n",
    "\n",
    "    for i in range(narray):\n",
    "        for j in range(narray):\n",
    "            outjet = decoded[narray*i+j,:,1:]\n",
    "            weights = decoded[narray*i+j,:,0]\n",
    "            pts, etas, phis = get_clustered_pt_eta_phi(weights, outjet,R=0.05)\n",
    "            x=codes[narray*i+j,sort_kl[dirs[0]]]\n",
    "            y=codes[narray*i+j,sort_kl[dirs[1]]]\n",
    "            if np.square(x) + np.square(y) > np.square(2.7):\n",
    "                ax[i,j].set_facecolor('xkcd:salmon')\n",
    "            ax[i,j].scatter(phis, etas, s = pts*1000, alpha = 0.7,linewidths=0)\n",
    "            ax[i,j].set_xlim(-0.5,0.5)\n",
    "            ax[i,j].set_ylim(-0.5,0.5)\n",
    "            ax[i,j].set_title('['+'{:.1f}'.format(x)+','+'{:.1f}'.format(y)+']',\n",
    "                             y=0.8)\n",
    "    #         ax[j,i].set_aspect('equal')\n",
    "    ax[int((narray-1)/2),int((narray-1)/2)].set_facecolor([0.9,0.9,0.9])\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    if save_dir:\n",
    "        outfile = os.path.join(output_dir,\"salmon_plot.png\".format(i))\n",
    "        plt.savefig(outfile)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_weights(file, save_dir = None,xlim=None,ylim=None):\n",
    "    vae.load_weights(file)\n",
    "    \n",
    "    if save_dir:\n",
    "        create_dir(save_dir)\n",
    "    \n",
    "    outs_array = [vae.predict(valid_x[:1000]) for j in range(3)]\n",
    "    _, z_mean, z_log_var, z = outs_array[0]\n",
    "\n",
    "    KL=kl_loss(z_mean, z_log_var)\n",
    "    sort_kl = np.flip(np.argsort(np.mean(KL,axis=0)))\n",
    "\n",
    "    rms_mean = np.sqrt(np.mean(np.square(z_mean),axis=0))\n",
    "\n",
    "    plt.scatter(np.mean(KL,axis=0),rms_mean,s=5.)\n",
    "\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "        \n",
    "    plt.xlabel('KL divergence')\n",
    "    plt.ylabel(r'$\\sqrt{\\left\\langle \\mu^2 \\right\\rangle}$')\n",
    "    plt.title(\"epoch = {}, beta = {}\".format(get_epoch(file),get_beta(file)))\n",
    "    if save_dir:\n",
    "        outfile = os.path.join(save_dir,\"KL_plot_{}_{}.png\".format(get_epoch(file),get_beta(file)))\n",
    "        plt.savefig(outfile)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42cbaa",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a860376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 150)\n",
      "Memory in GB: 2.250075340270996\n"
     ]
    }
   ],
   "source": [
    "# path to file\n",
    "fn =  '/global/home/users/yifengh3/data/B_background.h5'\n",
    "\n",
    "df = pandas.read_hdf(fn,stop=1000000)\n",
    "print(df.shape)\n",
    "print(\"Memory in GB:\",sum(df.memory_usage(deep=True)) / (1024**3)+sum(df.memory_usage(deep=True)) / (1024**3))\n",
    "\n",
    "# Data file contains, for each event, 50 particles (with zero padding), each particle with pT, eta, phi\n",
    "data = df.values.reshape((-1,50,3))\n",
    "\n",
    "# Normalize pTs so that HT = 1\n",
    "HT = np.sum(data[:,:,0],axis=-1)\n",
    "data[:,:,0] = data[:,:,0]/HT[:,None]\n",
    "\n",
    "# Inputs x to NN will be: pT, eta, cos(phi), sin(phi), log E\n",
    "# Separated phi into cos and sin for continuity around full detector, so make things easier for NN.\n",
    "# Also adding the log E is mainly because it seems like it should make things easier for NN, since there is an exponential spread in particle energies.\n",
    "# Feel free to change these choices as desired. E.g. px, py might be equally as good as pt, sin, cos.\n",
    "sig_input = np.zeros((len(data),50,4))\n",
    "sig_input[:,:,:2] = data[:,:,:2]\n",
    "sig_input[:,:,2] = np.cos(data[:,:,2])\n",
    "sig_input[:,:,3] = np.sin(data[:,:,2])\n",
    "# no input from energy for B jets\n",
    "# sig_input[:,:,4] = np.log(data[:,:,3]+1e-8)\n",
    "\n",
    "\n",
    "data_x = sig_input\n",
    "# Event 'labels' y are [pT, eta, phi], which is used to calculate EMD to output which is also pT, eta, phi.\n",
    "data_y = data\n",
    "\n",
    "\n",
    "train_x = data_x[:800000]\n",
    "train_y = data_y[:800000]\n",
    "valid_x = data_x[800000:]\n",
    "valid_y = data_y[800000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ac6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /global/home/users/yifengh3/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 50, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 50, 1024)     5120        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 50, 1024)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 50, 1024)     1049600     re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 50, 1024)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 50, 1028)     1053700     re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 50, 1028)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 50, 1024)     1053696     re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 50, 1024)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 1024)         0           re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1028)         1053700     tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 1028)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1028)         1057812     re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 1028)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1028)         1057812     re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 1028)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          526848      re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 128)          65664       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 128)          65664       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            multiple             6989616     inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack (TFOpLambda)           (2, None, 128)       0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 50, 3)        380360      encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gauss_distribution (Dis multiple             0           tf.stack[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,369,977\n",
      "Trainable params: 7,369,976\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_and_compile_annealing_vae.<locals>.recon_loss at 0x7fc8bc1f5040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_and_compile_annealing_vae.<locals>.recon_loss at 0x7fc8bc1f5040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method sinkhorn_knopp_tf_scaling_stabilized_class.do_dense of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc81c2f8ac0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method sinkhorn_knopp_tf_scaling_stabilized_class.do_dense of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc81c2f8ac0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method sinkhorn_knopp_tf_stabilized_alt_class.do_dense of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc81c2d1a60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method sinkhorn_knopp_tf_stabilized_alt_class.do_dense of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fc81c2d1a60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/global/home/users/yifengh3/B_result'\n",
    "experiment_name = 'B_train_1'\n",
    "train_output_dir = create_dir(osp.join(output_dir, experiment_name))\n",
    "vae, encoder, decoder = build_and_compile_annealing_vae(optimizer=keras.optimizers.Adam(lr=0.001,clipnorm=0.1),\n",
    "                                    encoder_conv_layers = [1024,1024,1028,1024],\n",
    "                                    dense_size = [1028,1028,1028,512],\n",
    "                                    decoder = [2048,2048,1028,512,512],\n",
    "                                    numItermaxinner = 40,   # EMD approximation params\n",
    "                                    numIter=10,\n",
    "                                    reg_init = 1.,\n",
    "                                    reg_final = 0.01,\n",
    "                                    stopThr=1e-3,\n",
    "                                    num_inputs=4,           # Size of x (e.g. pT, eta, sin, cos, log E)\n",
    "                                    num_particles_in=50)    # Num particles per event.\n",
    "\n",
    "batch_size=100\n",
    "save_period=2\n",
    "\n",
    "reduceLR = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, verbose=1, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0)\n",
    "modelcheckpoint = keras.callbacks.ModelCheckpoint(train_output_dir + '/model_weights_{epoch:02d}.hdf5', save_freq = save_period*5000, save_weights_only=True)\n",
    "reset_metrics_inst = reset_metrics()\n",
    "\n",
    "\n",
    "# Need to train on at least one example before model params can be loaded for annoying reasons.\n",
    "\n",
    "history = vae.fit(x=train_x[:10], y=train_y[:10], batch_size=batch_size,\n",
    "                epochs=1,verbose=1,#initial_epoch=int(vae.optimizer.iterations/numbatches),\n",
    "                validation_data = (valid_x[:10],valid_y[:10])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /global/home/users/yifengh3/B_result/B_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771fa36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Preparing to load weights\")\n",
    "\n",
    "files = glob.glob(train_output_dir + '/model_weights_end*.hdf5')\n",
    "files.sort(key=os.path.getmtime)\n",
    "epochs = np.array([get_epoch(file) for file in files])\n",
    "betas = np.array([get_beta(file) for file in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456dc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    load_weights(file,\"/global/home/users/yifengh3/B_result/B_train_1/weight_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96905741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf711a06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_plot(\"model_weights_end_620_2.1e-01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3cdbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_plot(\"model_weights_end_736_1.0e-01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a8d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_plot(\"model_weights_end_1823_2.0e-01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "outjet = valid_y[1999,:,1:]\n",
    "weights = valid_y[1999,:,0]\n",
    "pts, etas, phis = get_clustered_pt_eta_phi(weights, outjet,R=0.02)\n",
    "plt.hist(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c5b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa8cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_plot(\"model_weights_end_1788_4.9e-02.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832cbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_plot(\"model_weights_end_1803_1.0e-01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phis, etas, s = pts*1000, alpha = 0.7,linewidths=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5befc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"model_weights_end_1803_1.0e-01.hdf5\"\n",
    "vae.load_weights(file)\n",
    "beta = get_beta(file)\n",
    "outs_array = [vae.predict(valid_x[:1000]) for j in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outjet = outs_array[0][0][1,:,1:]\n",
    "weights = outs_array[0][0][1,:,0]\n",
    "pts, etas, phis = get_clustered_pt_eta_phi(weights, outjet,R=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf376cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outjet = valid_y[1,:,1:]\n",
    "weights = valid_y[1,:,0]\n",
    "pts, etas, phis = get_clustered_pt_eta_phi(weights, outjet,R=0.02)\n",
    "plt.hist(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90502e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
