{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef50705",
   "metadata": {},
   "source": [
    "# Import and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a505b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 15:52:59.338720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.datasets import qg_jets\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45667e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "train, val, test = 0.6, 0.3, 0.1\n",
    "Phi_sizes, F_sizes = (256, 256, 256), (256, 256, 256)\n",
    "num_epoch = 500\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aa5d4",
   "metadata": {},
   "source": [
    "# Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25276321",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_1_df = \"/global/home/users/yifengh3/VAE/new_data/muon_data/h_signal_big_muons.h5\"\n",
    "signal_2_df = \"/global/home/users/yifengh3/VAE/new_data/muon_data/hv_signal_big_muons.h5\"\n",
    "raw_signal_1 = pandas.read_hdf(signal_1_df)\n",
    "raw_signal_2 = pandas.read_hdf(signal_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646d74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173692, 200)\n",
      "signal_2 data shape: (157772, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"signal_1 data shape: {}\".format(raw_signal_1.shape))\n",
    "print(\"signal_2 data shape: {}\".format(raw_signal_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a75f65a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT0</th>\n",
       "      <th>eta0</th>\n",
       "      <th>phi0</th>\n",
       "      <th>E0</th>\n",
       "      <th>pT1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>phi1</th>\n",
       "      <th>E1</th>\n",
       "      <th>pT2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>...</th>\n",
       "      <th>phi47</th>\n",
       "      <th>E47</th>\n",
       "      <th>pT48</th>\n",
       "      <th>eta48</th>\n",
       "      <th>phi48</th>\n",
       "      <th>E48</th>\n",
       "      <th>pT49</th>\n",
       "      <th>eta49</th>\n",
       "      <th>phi49</th>\n",
       "      <th>E49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>40.94240</td>\n",
       "      <td>-1.718690</td>\n",
       "      <td>40.94240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>33.92570</td>\n",
       "      <td>-0.287077</td>\n",
       "      <td>33.92570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>4.71550</td>\n",
       "      <td>-0.290955</td>\n",
       "      <td>4.71550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>12.91650</td>\n",
       "      <td>-1.806460</td>\n",
       "      <td>12.91650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>16.97240</td>\n",
       "      <td>-1.327890</td>\n",
       "      <td>16.97240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6881</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>20.39380</td>\n",
       "      <td>-2.300500</td>\n",
       "      <td>20.39380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6882</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>8.02589</td>\n",
       "      <td>-1.974860</td>\n",
       "      <td>8.02589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>59.43760</td>\n",
       "      <td>0.043301</td>\n",
       "      <td>59.43760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>36.42490</td>\n",
       "      <td>1.648680</td>\n",
       "      <td>36.42490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>20.93080</td>\n",
       "      <td>0.580795</td>\n",
       "      <td>20.93080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173692 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pT0      eta0      phi0        E0  pT1  eta1  phi1   E1  pT2 eta2  ...  \\\n",
       "0    -13.0  40.94240 -1.718690  40.94240  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "1    -13.0  33.92570 -0.287077  33.92570  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "2    -13.0   4.71550 -0.290955   4.71550  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "3    -13.0  12.91650 -1.806460  12.91650  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "4    -13.0  16.97240 -1.327890  16.97240  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "...    ...       ...       ...       ...  ...   ...   ...  ...  ...  ...  ...   \n",
       "6881 -13.0  20.39380 -2.300500  20.39380  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "6882 -13.0   8.02589 -1.974860   8.02589  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "6883 -13.0  59.43760  0.043301  59.43760  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "6884 -13.0  36.42490  1.648680  36.42490  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "6885 -13.0  20.93080  0.580795  20.93080  0.0   0.0   0.0  0.0  NaN  NaN  ...   \n",
       "\n",
       "     phi47  E47 pT48 eta48 phi48  E48 pT49 eta49 phi49  E49  \n",
       "0      NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "1      NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "2      NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "3      NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "4      NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "...    ...  ...  ...   ...   ...  ...  ...   ...   ...  ...  \n",
       "6881   NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "6882   NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "6883   NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "6884   NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "6885   NaN  NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN  \n",
       "\n",
       "[173692 rows x 200 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_signal_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edf6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncate the nan\n",
    "signal_1 = raw_signal_1.to_numpy()[:,:8]\n",
    "signal_2 = raw_signal_2.to_numpy()[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6235130e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.0, 32.752112992999315, 0.001057640618451052,\n",
       "       32.752112992999315, 0.0, 0.0, 0.0, 0.0], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(signal_1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1b8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal_1 data shape: (173692, 8)\n",
      "signal_2 data shape: (157772, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"signal_1 data shape: {}\".format(signal_1.shape))\n",
    "print(\"signal_2 data shape: {}\".format(signal_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e582ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels to signal and background data, 0 for sig1,  1 for sig2\n",
    "# (updated since we might get multiple signals) \n",
    "labeled_sig1 = np.append(signal_1,np.zeros((signal_1.shape[0],1)),axis=1)\n",
    "labeled_sig2 = np.append(signal_2,np.ones((signal_2.shape[0],1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8540cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix two data array into one signal array\n",
    "data = np.concatenate((labeled_sig1,labeled_sig2))\n",
    "\n",
    "#and shuffle the data\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34eaf99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,:-1]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d078bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (331464, 8)\n",
      "shape of Y: (331464,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of X: {}\".format(X.shape))\n",
    "print(\"shape of Y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea34616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for background: 0.95\n",
      "Weight for signal: 1.05\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "total = labeled_sig1.shape[0] + labeled_sig2.shape[0]\n",
    "weight_for_0 = (1 / labeled_sig1.shape[0]) * (total / 2.0)\n",
    "weight_for_1 = (1 / labeled_sig2.shape[0]) * (total / 2.0)\n",
    "\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for background: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for signal: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e0e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To categorical as stipulated in example\n",
    "Y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Reshape X to shape (number of jets, 50, 4)\n",
    "X = X.reshape(-1,2,4)\n",
    "\n",
    "# ignore the pid info\n",
    "X = X[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f3e9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331464, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6fe36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalizing jets\n",
    "# # copied from example\n",
    "# import tqdm\n",
    "# for x in tqdm.tqdm(X):\n",
    "#     # now add the status bar :)\n",
    "#     mask = x[:,0] > 0\n",
    "#     yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "#     x[mask,1:3] -= yphi_avg\n",
    "#     x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e18f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing\n",
      "shape of X: (331464, 2, 3)\n",
      "shape of Y: (331464,)\n"
     ]
    }
   ],
   "source": [
    "print('Finished preprocessing')\n",
    "print(\"shape of X: {}\".format(X.shape))\n",
    "print(\"shape of Y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f573cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "939de622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do train/val/test split \n",
    "(X_train, X_val, X_test,\n",
    " Y_train, Y_val, Y_test) = data_split(X, Y, val=val, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6339cd",
   "metadata": {},
   "source": [
    "# Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4876d9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 15:53:40.451290: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-03 15:53:40.452453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-03 15:53:40.481862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:81:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-05-03 15:53:40.481884: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-03 15:53:40.483388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-03 15:53:40.483418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-03 15:53:40.484947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-03 15:53:40.485162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-03 15:53:40.486519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-03 15:53:40.487146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-03 15:53:40.489968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-03 15:53:40.492625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-03 15:53:40.493016: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-03 15:53:40.496185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:81:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-05-03 15:53:40.496204: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-03 15:53:40.496221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-03 15:53:40.496233: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-03 15:53:40.496244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-03 15:53:40.496255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-03 15:53:40.496265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-03 15:53:40.496276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-03 15:53:40.496286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-03 15:53:40.498750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-03 15:53:40.498796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-03 15:53:40.975553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-03 15:53:40.975602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-05-03 15:53:40.975612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-05-03 15:53:40.979588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22017 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:81:00.0, compute capability: 7.5)\n",
      "2022-05-03 15:53:40.979899: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 256)    1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    65792       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            514         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 330,498\n",
      "Trainable params: 330,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Model summary:')\n",
    "\n",
    "# build architecture\n",
    "pfn = PFN(input_dim=X.shape[-1], Phi_sizes=Phi_sizes, F_sizes=F_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b559b182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 15:53:41.319799: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-05-03 15:53:41.320266: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2994530000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/199 [..............................] - ETA: 1:49 - loss: 10.0800 - acc: 0.5320"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 15:53:41.715932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 2s 5ms/step - loss: 3.0631 - acc: 0.6979 - val_loss: 0.4942 - val_acc: 0.7704\n",
      "Epoch 2/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4983 - acc: 0.7703 - val_loss: 0.4813 - val_acc: 0.7779\n",
      "Epoch 3/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4895 - acc: 0.7718 - val_loss: 0.4801 - val_acc: 0.7789\n",
      "Epoch 4/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4885 - acc: 0.7715 - val_loss: 0.4822 - val_acc: 0.7789\n",
      "Epoch 5/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4866 - acc: 0.7730 - val_loss: 0.4813 - val_acc: 0.7779\n",
      "Epoch 6/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4893 - acc: 0.7723 - val_loss: 0.4792 - val_acc: 0.7790\n",
      "Epoch 7/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4852 - acc: 0.7746 - val_loss: 0.4814 - val_acc: 0.7782\n",
      "Epoch 8/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4866 - acc: 0.7728 - val_loss: 0.4814 - val_acc: 0.7783\n",
      "Epoch 9/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4871 - acc: 0.7729 - val_loss: 0.4795 - val_acc: 0.7789\n",
      "Epoch 10/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4862 - acc: 0.7736 - val_loss: 0.4889 - val_acc: 0.7761\n",
      "Epoch 11/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4858 - acc: 0.7741 - val_loss: 0.4796 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.000630957374449059.\n",
      "Epoch 12/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4852 - acc: 0.7738 - val_loss: 0.4845 - val_acc: 0.7774\n",
      "Epoch 13/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7749 - val_loss: 0.4810 - val_acc: 0.7780\n",
      "Epoch 14/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4830 - acc: 0.7754 - val_loss: 0.4801 - val_acc: 0.7787\n",
      "Epoch 15/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4835 - acc: 0.7754 - val_loss: 0.4867 - val_acc: 0.7770\n",
      "Epoch 16/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4843 - acc: 0.7745 - val_loss: 0.4816 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003981071838171537.\n",
      "Epoch 17/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4835 - acc: 0.7747 - val_loss: 0.4795 - val_acc: 0.7788\n",
      "Epoch 18/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4859 - acc: 0.7731 - val_loss: 0.4789 - val_acc: 0.7791\n",
      "Epoch 19/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4844 - acc: 0.7740 - val_loss: 0.4911 - val_acc: 0.7727\n",
      "Epoch 20/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4831 - acc: 0.7753 - val_loss: 0.4795 - val_acc: 0.7782\n",
      "Epoch 21/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4828 - acc: 0.7746 - val_loss: 0.4804 - val_acc: 0.7782\n",
      "Epoch 22/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4870 - acc: 0.7726 - val_loss: 0.4793 - val_acc: 0.7782\n",
      "Epoch 23/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4854 - acc: 0.7733 - val_loss: 0.4818 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002511886574257803.\n",
      "Epoch 24/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4853 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7788\n",
      "Epoch 25/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4842 - acc: 0.7730 - val_loss: 0.4790 - val_acc: 0.7787\n",
      "Epoch 26/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4840 - acc: 0.7741 - val_loss: 0.4805 - val_acc: 0.7776\n",
      "Epoch 27/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4830 - acc: 0.7745 - val_loss: 0.4803 - val_acc: 0.7780\n",
      "Epoch 28/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4839 - acc: 0.7743 - val_loss: 0.4842 - val_acc: 0.7782\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015848933651346973.\n",
      "Epoch 29/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4832 - acc: 0.7741 - val_loss: 0.4811 - val_acc: 0.7786\n",
      "Epoch 30/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4821 - acc: 0.7758 - val_loss: 0.4799 - val_acc: 0.7783\n",
      "Epoch 31/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4823 - acc: 0.7753 - val_loss: 0.4789 - val_acc: 0.7785\n",
      "Epoch 32/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4829 - acc: 0.7750 - val_loss: 0.4794 - val_acc: 0.7782\n",
      "Epoch 33/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4839 - acc: 0.7745 - val_loss: 0.4801 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000838432616.\n",
      "Epoch 34/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4818 - acc: 0.7742 - val_loss: 0.4797 - val_acc: 0.7781\n",
      "Epoch 35/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4809 - acc: 0.7764 - val_loss: 0.4798 - val_acc: 0.7780\n",
      "Epoch 36/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4823 - acc: 0.7752 - val_loss: 0.4789 - val_acc: 0.7788\n",
      "Epoch 37/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4839 - acc: 0.7734 - val_loss: 0.4821 - val_acc: 0.7764\n",
      "Epoch 38/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4854 - acc: 0.7734 - val_loss: 0.4789 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.30957374449059e-05.\n",
      "Epoch 39/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4844 - acc: 0.7738 - val_loss: 0.4813 - val_acc: 0.7779\n",
      "Epoch 40/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4832 - acc: 0.7758 - val_loss: 0.4837 - val_acc: 0.7768\n",
      "Epoch 41/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4823 - acc: 0.7755 - val_loss: 0.4800 - val_acc: 0.7784\n",
      "Epoch 42/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4822 - acc: 0.7753 - val_loss: 0.4799 - val_acc: 0.7786\n",
      "Epoch 43/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4821 - acc: 0.7746 - val_loss: 0.4804 - val_acc: 0.7782\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.981071838171537e-05.\n",
      "Epoch 44/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4823 - acc: 0.7744 - val_loss: 0.4797 - val_acc: 0.7783\n",
      "Epoch 45/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4817 - acc: 0.7750 - val_loss: 0.4810 - val_acc: 0.7777\n",
      "Epoch 46/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4831 - acc: 0.7741 - val_loss: 0.4810 - val_acc: 0.7778\n",
      "Epoch 47/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4817 - acc: 0.7754 - val_loss: 0.4803 - val_acc: 0.7788\n",
      "Epoch 48/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4824 - acc: 0.7748 - val_loss: 0.4789 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.5118865283496142e-05.\n",
      "Epoch 49/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4840 - acc: 0.7743 - val_loss: 0.4796 - val_acc: 0.7781\n",
      "Epoch 50/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4839 - acc: 0.7739 - val_loss: 0.4789 - val_acc: 0.7786\n",
      "Epoch 51/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4841 - acc: 0.7731 - val_loss: 0.4788 - val_acc: 0.7788\n",
      "Epoch 52/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4824 - acc: 0.7745 - val_loss: 0.4794 - val_acc: 0.7785\n",
      "Epoch 53/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4818 - acc: 0.7752 - val_loss: 0.4800 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.5848932274101303e-05.\n",
      "Epoch 54/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4835 - acc: 0.7742 - val_loss: 0.4800 - val_acc: 0.7778\n",
      "Epoch 55/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7753 - val_loss: 0.4795 - val_acc: 0.7786\n",
      "Epoch 56/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4817 - acc: 0.7747 - val_loss: 0.4800 - val_acc: 0.7785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4829 - acc: 0.7750 - val_loss: 0.4793 - val_acc: 0.7787\n",
      "Epoch 58/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4819 - acc: 0.7758 - val_loss: 0.4787 - val_acc: 0.7787\n",
      "Epoch 59/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4814 - acc: 0.7755 - val_loss: 0.4792 - val_acc: 0.7784\n",
      "Epoch 60/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4824 - acc: 0.7753 - val_loss: 0.4792 - val_acc: 0.7786\n",
      "Epoch 61/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4834 - acc: 0.7738 - val_loss: 0.4792 - val_acc: 0.7785\n",
      "Epoch 62/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4833 - acc: 0.7740 - val_loss: 0.4800 - val_acc: 0.7781\n",
      "Epoch 63/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4816 - acc: 0.7751 - val_loss: 0.4797 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.0000000608891671e-05.\n",
      "Epoch 64/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4843 - acc: 0.7737 - val_loss: 0.4790 - val_acc: 0.7784\n",
      "Epoch 65/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4825 - acc: 0.7746 - val_loss: 0.4796 - val_acc: 0.7780\n",
      "Epoch 66/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4820 - acc: 0.7750 - val_loss: 0.4794 - val_acc: 0.7782\n",
      "Epoch 67/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4836 - acc: 0.7731 - val_loss: 0.4796 - val_acc: 0.7782\n",
      "Epoch 68/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4820 - acc: 0.7750 - val_loss: 0.4794 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 69/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4839 - acc: 0.7735 - val_loss: 0.4790 - val_acc: 0.7786\n",
      "Epoch 70/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4834 - acc: 0.7734 - val_loss: 0.4792 - val_acc: 0.7785\n",
      "Epoch 71/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4816 - acc: 0.7752 - val_loss: 0.4797 - val_acc: 0.7783\n",
      "Epoch 72/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4835 - acc: 0.7746 - val_loss: 0.4797 - val_acc: 0.7783\n",
      "Epoch 73/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4841 - acc: 0.7732 - val_loss: 0.4793 - val_acc: 0.7782\n",
      "Epoch 74/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4818 - acc: 0.7747 - val_loss: 0.4795 - val_acc: 0.7784\n",
      "Epoch 75/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7748 - val_loss: 0.4790 - val_acc: 0.7786\n",
      "Epoch 76/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4808 - acc: 0.7751 - val_loss: 0.4794 - val_acc: 0.7783\n",
      "Epoch 77/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4820 - acc: 0.7756 - val_loss: 0.4794 - val_acc: 0.7783\n",
      "Epoch 78/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4815 - acc: 0.7755 - val_loss: 0.4792 - val_acc: 0.7783\n",
      "Epoch 79/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4813 - acc: 0.7750 - val_loss: 0.4801 - val_acc: 0.7779\n",
      "Epoch 80/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4823 - acc: 0.7749 - val_loss: 0.4796 - val_acc: 0.7784\n",
      "Epoch 81/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4789 - acc: 0.7766 - val_loss: 0.4790 - val_acc: 0.7787\n",
      "Epoch 82/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4841 - acc: 0.7739 - val_loss: 0.4788 - val_acc: 0.7788\n",
      "Epoch 83/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4812 - acc: 0.7763 - val_loss: 0.4802 - val_acc: 0.7779\n",
      "Epoch 84/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4842 - acc: 0.7744 - val_loss: 0.4789 - val_acc: 0.7784\n",
      "Epoch 85/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4845 - acc: 0.7734 - val_loss: 0.4794 - val_acc: 0.7784\n",
      "Epoch 86/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4806 - acc: 0.7757 - val_loss: 0.4797 - val_acc: 0.7782\n",
      "Epoch 87/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4827 - acc: 0.7741 - val_loss: 0.4791 - val_acc: 0.7786\n",
      "Epoch 88/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4823 - acc: 0.7743 - val_loss: 0.4795 - val_acc: 0.7783\n",
      "Epoch 89/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4817 - acc: 0.7753 - val_loss: 0.4795 - val_acc: 0.7781\n",
      "Epoch 90/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4817 - acc: 0.7751 - val_loss: 0.4795 - val_acc: 0.7781\n",
      "Epoch 91/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4824 - acc: 0.7750 - val_loss: 0.4797 - val_acc: 0.7782\n",
      "Epoch 92/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4831 - acc: 0.7738 - val_loss: 0.4790 - val_acc: 0.7789\n",
      "Epoch 93/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4835 - acc: 0.7756 - val_loss: 0.4793 - val_acc: 0.7785\n",
      "Epoch 94/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4823 - acc: 0.7749 - val_loss: 0.4804 - val_acc: 0.7779\n",
      "Epoch 95/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4816 - acc: 0.7756 - val_loss: 0.4798 - val_acc: 0.7780\n",
      "Epoch 96/500\n",
      "199/199 [==============================] - 1s 3ms/step - loss: 0.4835 - acc: 0.7738 - val_loss: 0.4793 - val_acc: 0.7784\n",
      "Epoch 97/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4790 - acc: 0.7775 - val_loss: 0.4797 - val_acc: 0.7782\n",
      "Epoch 98/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7745 - val_loss: 0.4792 - val_acc: 0.7785\n",
      "Epoch 99/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7741 - val_loss: 0.4791 - val_acc: 0.7784\n",
      "Epoch 100/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4833 - acc: 0.7745 - val_loss: 0.4793 - val_acc: 0.7782\n",
      "Epoch 101/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7745 - val_loss: 0.4790 - val_acc: 0.7784\n",
      "Epoch 102/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4797 - acc: 0.7761 - val_loss: 0.4793 - val_acc: 0.7785\n",
      "Epoch 103/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4828 - acc: 0.7742 - val_loss: 0.4791 - val_acc: 0.7784\n",
      "Epoch 104/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4813 - acc: 0.7760 - val_loss: 0.4789 - val_acc: 0.7787\n",
      "Epoch 105/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4818 - acc: 0.7748 - val_loss: 0.4797 - val_acc: 0.7784\n",
      "Epoch 106/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4817 - acc: 0.7746 - val_loss: 0.4797 - val_acc: 0.7779\n",
      "Epoch 107/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4841 - acc: 0.7733 - val_loss: 0.4796 - val_acc: 0.7782\n",
      "Epoch 108/500\n",
      "199/199 [==============================] - 1s 4ms/step - loss: 0.4830 - acc: 0.7747 - val_loss: 0.4799 - val_acc: 0.7782\n",
      "Epoch 00108: early stopping\n"
     ]
    }
   ],
   "source": [
    "# now train the model\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.1**(1/5), patience=5, min_lr=1e-5,\n",
    "                                                verbose=1)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, \n",
    "                                              verbose=1)\n",
    "\n",
    "callbacks = [reduce_lr,early_stop]\n",
    "\n",
    "hist1 = pfn.fit(X_train, Y_train,\n",
    "        epochs=num_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        class_weight=class_weight,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac1861",
   "metadata": {},
   "source": [
    "# Analyze the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531ee50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PFN AUC: 0.8387118129529184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions on test data\n",
    "preds = pfn.predict(X_test, batch_size=10000)\n",
    "\n",
    "# get ROC curve\n",
    "pfn_fp, pfn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1])\n",
    "\n",
    "# get area under the ROC curve\n",
    "auc = roc_auc_score(Y_test[:,1], preds[:,1])\n",
    "print()\n",
    "print('PFN AUC:', auc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ff086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get multiplicity and mass for comparison\n",
    "masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X])\n",
    "mults = np.asarray([np.count_nonzero(x[:,0]) for x in X])\n",
    "mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses)\n",
    "mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults)\n",
    "\n",
    "# some nicer plot settings \n",
    "plt.rcParams['figure.figsize'] = (4,4)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "\n",
    "# plot the ROC curves\n",
    "plt.plot(pfn_tp, 1-pfn_fp, '-', color='black', label='PFN')\n",
    "plt.plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass')\n",
    "plt.plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity')\n",
    "\n",
    "# axes labels\n",
    "plt.xlabel('Quark Jet Efficiency')\n",
    "plt.ylabel('Gluon Jet Rejection')\n",
    "\n",
    "# axes limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# make legend and show plot\n",
    "plt.legend(loc='lower left', frameon=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist1.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(hist1.history[\"val_loss\"],label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist1.history[\"acc\"],label=\"acc\")\n",
    "plt.plot(hist1.history[\"val_acc\"],label=\"validation acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1abd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
